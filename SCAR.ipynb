{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZvTt2Dmi+l+nyEdHe+J/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yazeedMohi/Christian-and-Berkeley-s-algorithms/blob/master/SCAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW8fvJCs3hmp",
        "colab_type": "text"
      },
      "source": [
        "# **SCAR (Scene Calibrated Arabic Recognizer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzhHyFOw34AJ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9jfAjbW3fcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SUX2XYu4BC5",
        "colab_type": "text"
      },
      "source": [
        "# Run Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbp3R1rg4DIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main(image=\"/content/drive/My Drive/data/demo/sign1.jpg\", localizer = \"east\",  recognizer = \"yazeed\", replacement=True, translation = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLzeCAVV4ELe",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipsoRa04GSQ",
        "colab_type": "code",
        "outputId": "87f3c0ec-6af1-4410-ff38-191abe6fd06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import os\n",
        "%tensorflow_version 2.1\n",
        "class CFG:\n",
        "    def __init__(self):\n",
        "        self.initial_epoch = 0\n",
        "        self.validation_split_ratio = 0.1\n",
        "        self.max_train_img_size = int(736)\n",
        "        self.max_predict_img_size = int(1536)\n",
        "        assert self.max_train_img_size in [256, 384, 512, 640, 736], \\\n",
        "            'max_train_img_size must in [256, 384, 512, 640, 736]'\n",
        "        if self.max_train_img_size == 256:\n",
        "            self.batch_size = 8\n",
        "        elif self.max_train_img_size == 384:\n",
        "            self.batch_size = 4\n",
        "        elif self.max_train_img_size == 512:\n",
        "            self.batch_size = 2\n",
        "        else:\n",
        "            self.batch_size = 1\n",
        "        \n",
        "        self.origin_image_dir_name = 'Images/Train/'\n",
        "        self.origin_txt_dir_name = 'GroundTruths/Train/'\n",
        "        self.origin_image_dir_name_test = 'Images/Test/'\n",
        "        self.origin_txt_dir_name_test = 'GroundTruths/Test/'\n",
        "        \n",
        "        self.shrink_ratio = 0.2\n",
        "        self.expand_ratio = -0.375\n",
        "        self.shrink_side_ratio = 0.6\n",
        "        self.epsilon = 1e-4\n",
        "\n",
        "        self.num_channels = 3\n",
        "        self.feature_layers_range = range(5, 1, -1)\n",
        "        self.feature_layers_num = len(self.feature_layers_range)\n",
        "        self.pixel_size = 2 ** self.feature_layers_range[-1]\n",
        "        self.locked_layers = False\n",
        "\n",
        "        \n",
        "        \n",
        "        self.pixel_threshold = 0.9\n",
        "        self.side_vertex_pixel_threshold = 0.9\n",
        "        self.trunc_threshold = 0.1\n",
        "        self.predict_cut_text_line = True\n",
        "        self.predict_write_txt = False\n",
        "\n",
        "        self.greedy=False\n",
        "        self.beam_width=10\n",
        "        self.top_paths=1\n",
        "\n",
        "        \n",
        "        self.show_process = False\n",
        "        self.Use_MedianFilter = True\n",
        "        self.translate_result = False\n",
        "        \n",
        "        # Post-Processing\n",
        "        self.segment_region_threshold = 10\n",
        "cfg = CFG()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.1`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6KjxhgK4T_7",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnw7GWzC4WlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import h5py\n",
        "import os\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "def Main(image,localizer = None,loc_source = \"icpr\", recognizer = None,rec_source=\"khattx\", replacement = False, translation = False):\n",
        "\n",
        "    img_path = image\n",
        "    assert(os.path.isfile(img_path))\n",
        "    if(localizer):\n",
        "        target_path = os.path.join(\"/content/drive/My Drive/\", \"output\", loc_source, localizer, \"checkpoint_weights.hdf5\")\n",
        "        loc_model = NNModel_loc(architecture=localizer)\n",
        "        loc_model.compile()\n",
        "        loc_model.load_checkpoint(target=target_path)\n",
        "\n",
        "        #print('3')\n",
        "    if(recognizer):\n",
        "        if(rec_source.find(\"khatt\") != -1):\n",
        "            arabic = True\n",
        "            # Best size for KHATT & KHATTX\n",
        "            input_size = (1024, 64, 1)\n",
        "            charset_base = 'ءآأإابتةثجحخدذرزسشصضطظعغفقكلمنؤهويىئ0123456789@:,.?!\"()//\\=-_#%$^&*+ '\n",
        "        else:\n",
        "            arabic = False\n",
        "            input_size = (1024, 128, 1)\n",
        "            charset_base = string.printable[:95]\n",
        "        max_text_length = 128\n",
        "        cfg.translate_result = translation\n",
        "        tokenizer = Tokenizer(chars=charset_base, max_text_length=max_text_length)\n",
        "        target_path = os.path.join(\"/content/drive/My Drive/\", \"output\", rec_source, recognizer, \"checkpoint_weights.hdf5\")\n",
        "        rec_model = NNModel_rec(architecture=recognizer,\n",
        "                                input_size=input_size,\n",
        "                                vocab_size=tokenizer.vocab_size,\n",
        "                                top_paths=10)\n",
        "        rec_model.compile()\n",
        "        rec_model.load_checkpoint(target=target_path) \n",
        "    \n",
        "    input_images = []\n",
        "    segmented = None\n",
        "\n",
        "    \n",
        "\n",
        "    geos = []\n",
        "    if(localizer):\n",
        "        print(\"Localizing...\")\n",
        "        input_images, geos, segmented = predict_image(loc_model.model, image, cfg.pixel_threshold)\n",
        "    else:\n",
        "        img = preprocess(cv2.imread(image), input_size=input_size)\n",
        "        input_images = [img]\n",
        "    if(recognizer):\n",
        "        print(\"Recognizing...\")\n",
        "        predicts, _ = predict_text(model = rec_model.model,x=input_images, ctc_decode=True)\n",
        "        predicts = [tokenizer.decode(x[0]) for x in predicts]\n",
        "        if(not replacement):\n",
        "          for pd, i in zip(predicts,range(len(predicts))):\n",
        "            cv2_imshow(adjust_to_see(input_images[i]))\n",
        "            print(str(pd))\n",
        "    if(replacement):\n",
        "        print(\"Replacing...\")\n",
        "        replace_all(img_path, geos, predicts)\n",
        "    else:\n",
        "        cv2_imshow(segmented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grlsMJwN4HNf",
        "colab_type": "text"
      },
      "source": [
        "# Localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699y_sRri2qB",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHeuj6u_i2P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "NNModel Class\n",
        "The NNModel class use Tensorflow Keras module.\n",
        "\n",
        "x is the input features and y the labels.\n",
        "\"\"\"\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Concatenate, Conv2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "##SET DEFS\n",
        "class NNModel_loc:\n",
        "\n",
        "    def __init__(self,\n",
        "                 architecture=\"east\",\n",
        "                 num_channels=3,\n",
        "                 locked_layers=False,\n",
        "                 feature_layers_range=range(5, 1, -1),\n",
        "                 feature_layers_num=4,\n",
        "                 learning_rate=1e-3\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialization of a NN Model.\n",
        "\n",
        "        parameters:\n",
        "            architecture: option of the architecture model to build and compile.\n",
        "        \"\"\"\n",
        "\n",
        "        self.architecture = globals()[architecture]\n",
        "        self.num_channels = num_channels\n",
        "        self.locked_layers = locked_layers\n",
        "        self.feature_layers_range = feature_layers_range\n",
        "        self.feature_layers_num = feature_layers_num\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.model= None\n",
        "\n",
        "    def summary(self, output=None, target=None):\n",
        "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        if target is not None:\n",
        "            os.makedirs(output, exist_ok=True)\n",
        "\n",
        "            \"\"\"with open(os.path.join(output, target), \"w\") as f:\n",
        "                with redirect_stdout(f):\n",
        "                    self.model.summary()\"\"\"\n",
        "\n",
        "    def load_checkpoint(self, target):\n",
        "        \"\"\" Load a model with checkpoint file\"\"\"\n",
        "\n",
        "        if os.path.isfile(target):\n",
        "            if self.model is None:\n",
        "                self.compile()\n",
        "\n",
        "            self.model.load_weights(target)\n",
        "\n",
        "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
        "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
        "\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                filename=os.path.join(logdir, \"epochs.log\"),\n",
        "                separator=\";\",\n",
        "                append=True),\n",
        "            TensorBoard(\n",
        "                log_dir=logdir,\n",
        "                write_graph=True,\n",
        "                write_images=False,\n",
        "                update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(\n",
        "                filepath=checkpoint,\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=verbose),\n",
        "            EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                factor=0.2,\n",
        "                patience=15,\n",
        "                verbose=verbose)\n",
        "        ]\n",
        "\n",
        "\n",
        "        return callbacks\n",
        "    def compile(self, learning_rate=None):\n",
        "        \"\"\"\n",
        "        Configures the NN Model for training/predict.\n",
        "\n",
        "        optimizer: optimizer for training\n",
        "        \"\"\"\n",
        "\n",
        "        # define inputs, outputs and optimizer of the chosen architecture\n",
        "        outs = self.architecture(self.num_channels, self.locked_layers, self.feature_layers_range, self.feature_layers_num, self.learning_rate)\n",
        "        inputs, outputs, optimizer = outs\n",
        "\n",
        "        # create and compile\n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.quad_loss)\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            validation_split=0.0,\n",
        "            validation_data=None,\n",
        "            shuffle=True,\n",
        "            class_weight=None,\n",
        "            sample_weight=None,\n",
        "            initial_epoch=0,\n",
        "            steps_per_epoch=None,\n",
        "            validation_steps=None,\n",
        "            validation_freq=1,\n",
        "            max_queue_size=10,\n",
        "            workers=1,\n",
        "            use_multiprocessing=False,\n",
        "            checkpoint=\"\"):\n",
        "        \"\"\"\n",
        "        Model training on data yielded (fit_generator function has support to generator).\n",
        "        A fit_generator() abstration function of TensorFlow Keras.\n",
        "\n",
        "        Provide x parameter, a generator of the form: yielding x, y.\n",
        "\n",
        "        return: A history object\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.model.fit_generator(generator=x,\n",
        "                           steps_per_epoch=steps_per_epoch,\n",
        "                           epochs=epochs,\n",
        "                           validation_data=validation_data,\n",
        "                           validation_steps=validation_steps,\n",
        "                           verbose=1,\n",
        "                           initial_epoch=initial_epoch,\n",
        "                           callbacks=callbacks)\n",
        "        return out\n",
        "\n",
        "    def predict(self,\n",
        "                x,\n",
        "                batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def quad_loss(y_true, y_pred):\n",
        "        \n",
        "        epsilon = 1e-6\n",
        "        lambda_side_vertex_coord_loss = 1.0\n",
        "        lambda_side_vertex_code_loss = 1.0\n",
        "        lambda_inside_score_loss = 4.0\n",
        "        def smooth_l1_loss(prediction_tensor, target_tensor, weights):\n",
        "            n_q = tf.reshape(quad_norm(target_tensor), tf.shape(weights))\n",
        "            diff = prediction_tensor - target_tensor\n",
        "            abs_diff = tf.abs(diff)\n",
        "            abs_diff_lt_1 = tf.less(abs_diff, 1)\n",
        "            pixel_wise_smooth_l1norm = (tf.reduce_sum(\n",
        "                tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5),\n",
        "                axis=-1) / n_q) * weights\n",
        "            return pixel_wise_smooth_l1norm\n",
        "\n",
        "\n",
        "        def quad_norm(g_true):\n",
        "            shape = tf.shape(g_true)\n",
        "            delta_xy_matrix = tf.reshape(g_true, [-1, 2, 2])\n",
        "            diff = delta_xy_matrix[:, 0:1, :] - delta_xy_matrix[:, 1:2, :]\n",
        "            square = tf.square(diff)\n",
        "            distance = tf.sqrt(tf.reduce_sum(square, axis=-1))\n",
        "            distance *= 4.0\n",
        "            distance += epsilon\n",
        "            return tf.reshape(distance, shape[:-1])\n",
        "        \n",
        "        \n",
        "        # loss for inside_score\n",
        "        logits = y_pred[:, :, :, :1]\n",
        "        labels = y_true[:, :, :, :1]\n",
        "        # balance positive and negative samples in an image\n",
        "        beta = 1 - tf.reduce_mean(labels)\n",
        "        # first apply sigmoid activation\n",
        "        predicts = tf.nn.sigmoid(logits)\n",
        "        # log +epsilon for stable cal\n",
        "        inside_score_loss = tf.reduce_mean(\n",
        "            -1 * (beta * labels * tf.math.log(predicts + epsilon) +\n",
        "                  (1 - beta) * (1 - labels) * tf.math.log(1 - predicts + epsilon)))\n",
        "        inside_score_loss *= lambda_inside_score_loss\n",
        "\n",
        "        # loss for side_vertex_code\n",
        "        vertex_logits = y_pred[:, :, :, 1:3]\n",
        "        vertex_labels = y_true[:, :, :, 1:3]\n",
        "        vertex_beta = 1 - (tf.reduce_mean(y_true[:, :, :, 1:2])\n",
        "                           / (tf.reduce_mean(labels) + epsilon))\n",
        "        vertex_predicts = tf.nn.sigmoid(vertex_logits)\n",
        "        pos = -1 * vertex_beta * vertex_labels * tf.math.log(vertex_predicts +\n",
        "                                                        epsilon)\n",
        "        neg = -1 * (1 - vertex_beta) * (1 - vertex_labels) * tf.math.log(\n",
        "            1 - vertex_predicts + epsilon)\n",
        "        positive_weights = tf.cast(tf.equal(y_true[:, :, :, 0], 1), tf.float32)\n",
        "        side_vertex_code_loss = \\\n",
        "            tf.reduce_sum(tf.reduce_sum(pos + neg, axis=-1) * positive_weights) / (\n",
        "                    tf.reduce_sum(positive_weights) + epsilon)\n",
        "        side_vertex_code_loss *= lambda_side_vertex_code_loss\n",
        "\n",
        "        # loss for side_vertex_coord delta\n",
        "        g_hat = y_pred[:, :, :, 3:]\n",
        "        g_true = y_true[:, :, :, 3:]\n",
        "        vertex_weights = tf.cast(tf.equal(y_true[:, :, :, 1], 1), tf.float32)\n",
        "        pixel_wise_smooth_l1norm = smooth_l1_loss(g_hat, g_true, vertex_weights)\n",
        "        side_vertex_coord_loss = tf.reduce_sum(pixel_wise_smooth_l1norm) / (\n",
        "                tf.reduce_sum(vertex_weights) + epsilon)\n",
        "        side_vertex_coord_loss *= lambda_side_vertex_coord_loss\n",
        "        \n",
        "        return inside_score_loss + side_vertex_code_loss + side_vertex_coord_loss\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Networks to the text localization\n",
        "\"\"\"\n",
        "\n",
        "def east(num_channels, locked_layers, feature_layers_range, feature_layers_num, learning_rate):\n",
        "    input_img = Input(name='input_img',\n",
        "                               shape=(None, None, num_channels),\n",
        "                               dtype='float32')\n",
        "    vgg16 = VGG16(input_tensor=input_img,\n",
        "                  weights='imagenet',\n",
        "                  include_top=False)\n",
        "    if locked_layers:\n",
        "        # locked first two conv layers\n",
        "        locked_layers = [vgg16.get_layer('block1_conv1'),\n",
        "                         vgg16.get_layer('block1_conv2')]\n",
        "        for layer in locked_layers:\n",
        "            layer.trainable = False\n",
        "    f = [vgg16.get_layer('block%d_pool' % i).output\n",
        "              for i in feature_layers_range]\n",
        "    f.insert(0, None)\n",
        "    diff = feature_layers_range[0] - feature_layers_num\n",
        "    \n",
        "    def g(i):\n",
        "        # i+diff in cfg.feature_layers_range\n",
        "        assert i + diff in feature_layers_range, \\\n",
        "            ('i=%d+diff=%d not in ' % (i, diff)) + \\\n",
        "            str(feature_layers_range)\n",
        "        if i == feature_layers_num:\n",
        "            bn = BatchNormalization()(h(i))\n",
        "            cn = Conv2D(32, 3, activation='relu', padding='same')(bn)\n",
        "            return cn\n",
        "        else:\n",
        "            return UpSampling2D((2, 2))(h(i))\n",
        "\n",
        "    def h(i):\n",
        "        # i+diff in cfg.feature_layers_range\n",
        "        assert i + diff in feature_layers_range, \\\n",
        "            ('i=%d+diff=%d not in ' % (i, diff)) + \\\n",
        "            str(feature_layers_range)\n",
        "        if i == 1:\n",
        "            return f[i]\n",
        "        else:\n",
        "            concat = Concatenate(axis=-1)([g(i - 1), f[i]])\n",
        "            bn1 = BatchNormalization()(concat)\n",
        "            conv_1 = Conv2D(128 // 2 ** (i - 2), 1,\n",
        "                            activation='relu', padding='same',)(bn1)\n",
        "            bn2 = BatchNormalization()(conv_1)\n",
        "            conv_3 = Conv2D(128 // 2 ** (i - 2), 3,\n",
        "                            activation='relu', padding='same',)(bn2)\n",
        "            return conv_3\n",
        "\n",
        "    before_output = g(feature_layers_num)\n",
        "    inside_score = Conv2D(1, 1, padding='same', name='inside_score'\n",
        "                          )(before_output)\n",
        "    side_v_code = Conv2D(2, 1, padding='same', name='side_vertex_code'\n",
        "                         )(before_output)\n",
        "    side_v_coord = Conv2D(4, 1, padding='same', name='side_vertex_coord'\n",
        "                          )(before_output)\n",
        "    east_detect = Concatenate(axis=-1,\n",
        "                              name='east_detect')([inside_score,\n",
        "                                                   side_v_code,\n",
        "                                                   side_v_coord])\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate, decay=5e-4)\n",
        "    \n",
        "    return (input_img, east_detect, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dYnbAdb6tgw",
        "colab_type": "text"
      },
      "source": [
        "## Predict Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGdCWpOP6sUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from PIL import ImageQt\n",
        "from keras.preprocessing import image as image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"`y = 1 / (1 + exp(-x))`\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def square_geo(geo):\n",
        "    result = geo.copy()\n",
        "    p_min = np.min(geo,axis=0)\n",
        "    p_max = np.max(geo,axis=0)\n",
        "    result = [[p_min[0],p_min[1]],[p_min[0],p_max[1]],[p_max[0],p_max[1]],[p_max[0],p_min[1]]]\n",
        "    return np.asarray(result)\n",
        "def cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array, img_path, s):\n",
        "    geo /= [scale_ratio_w, scale_ratio_h]\n",
        "    \n",
        "    _, geo, _ = shrink(geo,ratio = cfg.expand_ratio)\n",
        "    geo_orig = geo.copy()\n",
        "    geo = square_geo(geo)\n",
        "    #print(\"GEO: \",geo)\n",
        "    p_min = np.amin(geo, axis=0)\n",
        "    p_max = np.amax(geo, axis=0)\n",
        "    min_xy = p_min.astype(int)\n",
        "    max_xy = p_max.astype(int) + 2\n",
        "    sub_im_arr = im_array[min_xy[1]:max_xy[1], min_xy[0]:max_xy[0], :].copy()\n",
        "    return binarize(sub_im_arr,geo_orig)\n",
        "    \n",
        "def predict_image(detector, img_path, pixel_threshold, quiet=True,gen=None):\n",
        "    results = []\n",
        "    geos = []\n",
        "    #print(img_path.find(\"sample\"),img_path.find(\"Out\"))\n",
        "    if(img_path.find(\"sample\")==0):\n",
        "        im2,out = gen.sample()\n",
        "        img = image.array_to_img(im2)\n",
        "    else:\n",
        "        img = image.load_img(img_path)\n",
        "\n",
        "\n",
        "    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "    img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    img = image.img_to_array(img)\n",
        "    \n",
        "    img = preprocess_input(img, mode='tf')\n",
        "\n",
        "    b, g, r = img[:,:,0],img[:,:,1],img[:,:,2]\n",
        "    img = np.dstack([r,g,b])\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    #print(x.shape)\n",
        "    if(img_path.find(\"Out\")==6):\n",
        "        y = out\n",
        "    else:\n",
        "        y = detector.predict(x)\n",
        "        y = np.squeeze(y, axis=0)\n",
        "    #print(y.shape)#,y[:,:,0],y[:,:,1],y[:,:,2],y[:,:,3],y[:,:,4],y[:,:,5],y[:,:,6])\n",
        "    y[:, :, :3] = sigmoid(y[:, :, :3])\n",
        "    opt = np.get_printoptions()\n",
        "    \"\"\"np.set_printoptions(threshold=np.inf)\n",
        "    #print(np.asarray(y[:,:,1]))\n",
        "    print(np.asarray(y[:,:,2]))\n",
        "    np.set_printoptions(**opt)\"\"\"\n",
        "    cond = np.greater_equal(y[:, :, 0],sigmoid(pixel_threshold))\n",
        "    activation_pixels = np.where(cond)\n",
        "    quad_scores, quad_after_nms = nms(y, activation_pixels)\n",
        "        #im = im2\n",
        "    im = image.array_to_img(img)\n",
        "    im_array = image.img_to_array(im.convert('RGB'))\n",
        "    d_wight, d_height = resize_image(im, cfg.max_predict_img_size)\n",
        "    scale_ratio_w = d_wight / im.width\n",
        "    scale_ratio_h = d_height / im.height\n",
        "    im = im.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    quad_im = im.copy()\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    for i, j in zip(activation_pixels[0], activation_pixels[1]):\n",
        "        px = (j + 0.5) * cfg.pixel_size\n",
        "        py = (i + 0.5) * cfg.pixel_size\n",
        "        line_width, line_color = 1, 'red'\n",
        "        if y[i, j, 1] >= sigmoid(cfg.side_vertex_pixel_threshold):\n",
        "            if y[i, j, 2] < sigmoid(cfg.trunc_threshold):\n",
        "                line_width, line_color = 2, 'yellow'\n",
        "            elif y[i, j, 2] >= sigmoid(1-cfg.trunc_threshold):\n",
        "                line_width, line_color = 2, 'green'\n",
        "        draw.line([(px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n",
        "                    (px + 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n",
        "                    (px + 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n",
        "                    (px - 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n",
        "                    (px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size)],\n",
        "                  width=line_width, fill=line_color)\n",
        "    quad_draw = ImageDraw.Draw(quad_im)\n",
        "    txt_items = []\n",
        "    \n",
        "    for score, geo, s in zip(quad_scores, quad_after_nms,\n",
        "                              range(len(quad_scores))):\n",
        "        if np.amin(score) > sigmoid(0):\n",
        "            _,geo2,_ = shrink(geo,ratio = cfg.expand_ratio)\n",
        "            quad_draw.line([tuple(geo2[0]),\n",
        "                            tuple(geo2[1]),\n",
        "                            tuple(geo2[2]),\n",
        "                            tuple(geo2[3]),\n",
        "                            tuple(geo2[0])], width=2, fill='red')\n",
        "            if cfg.predict_cut_text_line:\n",
        "                x = cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array,\n",
        "                              img_path, s)\n",
        "                if x is not None:\n",
        "                    results.append(x)\n",
        "                    geos.append(geo)\n",
        "            rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n",
        "            rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n",
        "            txt_item = ','.join(map(str, rescaled_geo_list))\n",
        "            txt_items.append(txt_item + '\\n')\n",
        "        elif not quiet:\n",
        "            print(np.amin(score))\n",
        "            print(score)\n",
        "            print('quad invalid with vertex num less then 4.')\n",
        "    \n",
        "    #cv2_imshow(image.img_to_array(im.convert('RGB')))\n",
        "    #cv2_imshow(image.img_to_array(quad_im.convert('RGB')))\n",
        "    #print(len(results))\n",
        "    return results, geos, image.img_to_array(quad_im.convert('RGB'))\n",
        "\n",
        "\n",
        "def predict_txt(east_detect, img_path, txt_path, pixel_threshold, quiet=False):\n",
        "    img = image.load_img(img_path)\n",
        "    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "    scale_ratio_w = d_wight / img.width\n",
        "    scale_ratio_h = d_height / img.height\n",
        "    img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    img = image.img_to_array(img)\n",
        "    img = preprocess_input(img, mode='tf')\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    y = east_detect.predict(x)\n",
        "\n",
        "    y = np.squeeze(y, axis=0)\n",
        "    y[:, :, :3] = sigmoid(y[:, :, :3])\n",
        "    cond = np.greater_equal(y[:, :, 0], pixel_threshold)\n",
        "    activation_pixels = np.where(cond)\n",
        "    quad_scores, quad_after_nms = nms(y, activation_pixels)\n",
        "\n",
        "    txt_items = []\n",
        "    for score, geo in zip(quad_scores, quad_after_nms):\n",
        "        if np.amin(score) > 0:\n",
        "            rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n",
        "            rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n",
        "            txt_item = ','.join(map(str, rescaled_geo_list))\n",
        "            txt_items.append(txt_item + '\\n')\n",
        "        elif not quiet:\n",
        "            print('quad invalid with vertex num less then 4.')\n",
        "    if cfg.predict_write2txt and len(txt_items) > 0:\n",
        "        with open(txt_path, 'w') as f_txt:\n",
        "            f_txt.writelines(txt_items)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--path', '-p',\n",
        "                        default='demo/012.png',\n",
        "                        help='image path')\n",
        "    parser.add_argument('--threshold', '-t',\n",
        "                        default=cfg.pixel_threshold,\n",
        "                        help='pixel activation threshold')\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4lZxAo4l3N0",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w6Q4Ym5l1e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_reorder_vertexes(xy_list_array):\n",
        "    reorder_xy_list_array = np.zeros_like(xy_list_array)\n",
        "    for xy_list, i in zip(xy_list_array, range(len(xy_list_array))):\n",
        "        reorder_xy_list_array[i] = reorder_vertexes(xy_list)\n",
        "    return reorder_xy_list_array\n",
        "\n",
        "\n",
        "def reorder_vertexes(xy_list):\n",
        "    reorder_xy_list = np.zeros_like(xy_list)\n",
        "    # determine the first point with the smallest x,\n",
        "    # if two has same x, choose that with smallest y,\n",
        "    ordered = np.argsort(xy_list, axis=0)\n",
        "    xmin1_index = ordered[0, 0]\n",
        "    xmin2_index = ordered[1, 0]\n",
        "    if xy_list[xmin1_index, 0] == xy_list[xmin2_index, 0]:\n",
        "        if xy_list[xmin1_index, 1] <= xy_list[xmin2_index, 1]:\n",
        "            reorder_xy_list[0] = xy_list[xmin1_index]\n",
        "            first_v = xmin1_index\n",
        "        else:\n",
        "            reorder_xy_list[0] = xy_list[xmin2_index]\n",
        "            first_v = xmin2_index\n",
        "    else:\n",
        "        reorder_xy_list[0] = xy_list[xmin1_index]\n",
        "        first_v = xmin1_index\n",
        "    # connect the first point to others, the third point on the other side of\n",
        "    # the line with the middle slope\n",
        "    others = list(range(4))\n",
        "    others.remove(first_v)\n",
        "    k = np.zeros((len(others),))\n",
        "    for index, i in zip(others, range(len(others))):\n",
        "        k[i] = (xy_list[index, 1] - xy_list[first_v, 1]) \\\n",
        "                    / (xy_list[index, 0] - xy_list[first_v, 0] + cfg.epsilon)\n",
        "    k_mid = np.argsort(k)[1]\n",
        "    third_v = others[k_mid]\n",
        "    reorder_xy_list[2] = xy_list[third_v]\n",
        "    # determine the second point which on the bigger side of the middle line\n",
        "    others.remove(third_v)\n",
        "    b_mid = xy_list[first_v, 1] - k[k_mid] * xy_list[first_v, 0]\n",
        "    second_v, fourth_v = 0, 0\n",
        "    for index, i in zip(others, range(len(others))):\n",
        "        # delta = y - (k * x + b)\n",
        "        delta_y = xy_list[index, 1] - (k[k_mid] * xy_list[index, 0] + b_mid)\n",
        "        if delta_y > 0:\n",
        "            second_v = index\n",
        "        else:\n",
        "            fourth_v = index\n",
        "    reorder_xy_list[1] = xy_list[second_v]\n",
        "    reorder_xy_list[3] = xy_list[fourth_v]\n",
        "    # compare slope of 13 and 24, determine the final order\n",
        "    k13 = k[k_mid]\n",
        "    k24 = (xy_list[second_v, 1] - xy_list[fourth_v, 1]) / (\n",
        "                xy_list[second_v, 0] - xy_list[fourth_v, 0] + cfg.epsilon)\n",
        "    if k13 < k24:\n",
        "        tmp_x, tmp_y = reorder_xy_list[3, 0], reorder_xy_list[3, 1]\n",
        "        for i in range(2, -1, -1):\n",
        "            reorder_xy_list[i + 1] = reorder_xy_list[i]\n",
        "        reorder_xy_list[0, 0], reorder_xy_list[0, 1] = tmp_x, tmp_y\n",
        "    return reorder_xy_list\n",
        "\n",
        "\n",
        "def resize_image(im, max_img_size=cfg.max_train_img_size):\n",
        "    im_width = np.minimum(im.width, max_img_size)\n",
        "    if im_width == max_img_size < im.width:\n",
        "        im_height = int((im_width / im.width) * im.height)\n",
        "    else:\n",
        "        im_height = im.height\n",
        "    o_height = np.minimum(im_height, max_img_size)\n",
        "    if o_height == max_img_size < im_height:\n",
        "        o_width = int((o_height / im_height) * im_width)\n",
        "    else:\n",
        "        o_width = im_width\n",
        "    d_wight = o_width - (o_width % 32)\n",
        "    d_height = o_height - (o_height % 32)\n",
        "    return d_wight, d_height\n",
        "\n",
        "\n",
        "def point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n",
        "    if (p_min[0] <= px <= p_max[0]) and (p_min[1] <= py <= p_max[1]):\n",
        "        xy_list = np.zeros((4, 2))\n",
        "        xy_list[:3, :] = quad_xy_list[1:4, :] - quad_xy_list[:3, :]\n",
        "        xy_list[3] = quad_xy_list[0, :] - quad_xy_list[3, :]\n",
        "        yx_list = np.zeros((4, 2))\n",
        "        yx_list[:, :] = quad_xy_list[:, -1:-3:-1]\n",
        "        a = xy_list * ([py, px] - yx_list)\n",
        "        b = a[:, 0] - a[:, 1]\n",
        "        if np.amin(b) >= 0 or np.amax(b) <= 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def point_inside_of_nth_quad(px, py, xy_list, shrink_1, long_edge):\n",
        "    nth = -1\n",
        "    vs = [[[0, 0, 3, 3, 0], [1, 1, 2, 2, 1]],\n",
        "          [[0, 0, 1, 1, 0], [2, 2, 3, 3, 2]]]\n",
        "    for ith in range(2):\n",
        "        quad_xy_list = np.concatenate((\n",
        "            np.reshape(xy_list[vs[long_edge][ith][0]], (1, 2)),\n",
        "            np.reshape(shrink_1[vs[long_edge][ith][1]], (1, 2)),\n",
        "            np.reshape(shrink_1[vs[long_edge][ith][2]], (1, 2)),\n",
        "            np.reshape(xy_list[vs[long_edge][ith][3]], (1, 2))), axis=0)\n",
        "        p_min = np.amin(quad_xy_list, axis=0)\n",
        "        p_max = np.amax(quad_xy_list, axis=0)\n",
        "        if point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n",
        "            if nth == -1:\n",
        "                nth = ith\n",
        "            else:\n",
        "                nth = -1\n",
        "                break\n",
        "    return nth\n",
        "\n",
        "\n",
        "def shrink(xy_list, ratio=cfg.shrink_ratio):\n",
        "    if ratio == 0.0:\n",
        "        return xy_list, xy_list\n",
        "    diff_1to3 = xy_list[:3, :] - xy_list[1:4, :]\n",
        "    diff_4 = xy_list[3:4, :] - xy_list[0:1, :]\n",
        "    diff = np.concatenate((diff_1to3, diff_4), axis=0)\n",
        "    dis = np.sqrt(np.sum(np.square(diff), axis=-1))\n",
        "    # determine which are long or short edges\n",
        "    long_edge = int(np.argmax(np.sum(np.reshape(dis, (2, 2)), axis=0)))\n",
        "    short_edge = 1 - long_edge\n",
        "    # cal r length array\n",
        "    r = [np.minimum(dis[i], dis[(i + 1) % 4]) for i in range(4)]\n",
        "    # cal theta array\n",
        "    diff_abs = np.abs(diff)\n",
        "    diff_abs[:, 0] += cfg.epsilon\n",
        "    theta = np.arctan(diff_abs[:, 1] / diff_abs[:, 0])\n",
        "    # shrink two long edges\n",
        "    temp_new_xy_list = np.copy(xy_list)\n",
        "    shrink_edge(xy_list, temp_new_xy_list, long_edge, r, theta, ratio)\n",
        "    shrink_edge(xy_list, temp_new_xy_list, long_edge + 2, r, theta, ratio)\n",
        "    # shrink two short edges\n",
        "    new_xy_list = np.copy(temp_new_xy_list)\n",
        "    shrink_edge(temp_new_xy_list, new_xy_list, short_edge, r, theta, ratio)\n",
        "    shrink_edge(temp_new_xy_list, new_xy_list, short_edge + 2, r, theta, ratio)\n",
        "    return temp_new_xy_list, new_xy_list, long_edge\n",
        "\n",
        "\n",
        "def shrink_edge(xy_list, new_xy_list, edge, r, theta, ratio=cfg.shrink_ratio):\n",
        "    if ratio == 0.0:\n",
        "        return\n",
        "    start_point = edge\n",
        "    end_point = (edge + 1) % 4\n",
        "    long_start_sign_x = np.sign(\n",
        "        xy_list[end_point, 0] - xy_list[start_point, 0])\n",
        "    new_xy_list[start_point, 0] = \\\n",
        "        xy_list[start_point, 0] + \\\n",
        "        long_start_sign_x * ratio * r[start_point] * np.cos(theta[start_point])\n",
        "    long_start_sign_y = np.sign(\n",
        "        xy_list[end_point, 1] - xy_list[start_point, 1])\n",
        "    new_xy_list[start_point, 1] = \\\n",
        "        xy_list[start_point, 1] + \\\n",
        "        long_start_sign_y * ratio * r[start_point] * np.sin(theta[start_point])\n",
        "    # long edge one, end point\n",
        "    long_end_sign_x = -1 * long_start_sign_x\n",
        "    new_xy_list[end_point, 0] = \\\n",
        "        xy_list[end_point, 0] + \\\n",
        "        long_end_sign_x * ratio * r[end_point] * np.cos(theta[start_point])\n",
        "    long_end_sign_y = -1 * long_start_sign_y\n",
        "    new_xy_list[end_point, 1] = \\\n",
        "        xy_list[end_point, 1] + \\\n",
        "        long_end_sign_y * ratio * r[end_point] * np.sin(theta[start_point])\n",
        "\n",
        "\n",
        "arabic = True\n",
        "def adjust_to_see(img):\n",
        "    \"\"\"Rotate and transpose to image visualize (cv2 method or jupyter notebook)\"\"\"\n",
        "\n",
        "    (h, w) = img.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -90, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    img = cv2.warpAffine(img, M, (nW + 1, nH + 1))\n",
        "    img = cv2.warpAffine(img.transpose(), M, (nW, nH))\n",
        "    if arabic:\n",
        "      img = cv2.flip(img,1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def augmentation(imgs,\n",
        "                 rotation_range=0,\n",
        "                 scale_range=0,\n",
        "                 height_shift_range=0,\n",
        "                 width_shift_range=0,\n",
        "                 dilate_range=1,\n",
        "                 erode_range=1):\n",
        "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
        "\n",
        "    imgs = imgs.astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
        "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
        "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
        "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
        "    scale = np.random.uniform(1 - scale_range, 1)\n",
        "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
        "\n",
        "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
        "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
        "\n",
        "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
        "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
        "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
        "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
        "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def normalization(imgs):\n",
        "    \"\"\"Normalize list of images\"\"\"\n",
        "    #print(imgs[0].shape)\n",
        "    imgs = np.asarray(imgs).astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        m, s = cv2.meanStdDev(imgs[i])\n",
        "        imgs[i] = imgs[i] - m[0][0]\n",
        "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
        "    \n",
        "    return np.expand_dims(imgs, axis=-1)\n",
        "\n",
        "def preprocess(img, input_size):\n",
        "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
        "    \n",
        "    if isinstance(img, str):\n",
        "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if isinstance(img, tuple):\n",
        "        image, boundbox = img\n",
        "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        for i in range(len(boundbox)):\n",
        "            if isinstance(boundbox[i], float):\n",
        "                total = len(img) if i < 2 else len(img[0])\n",
        "                boundbox[i] = int(total * boundbox[i])\n",
        "\n",
        "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
        "    wt, ht, _ = input_size\n",
        "    h, w = np.asarray(img).shape[:2]\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "    \n",
        "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Not necessary for the KHATTX dataset, uncomment for IAM or RIMES.\n",
        "    \n",
        "    #if np.sum(img) * 0.8 > np.sum(binary):\n",
        "    #    img = illumination_compensation(img)\n",
        "\n",
        "    #img = remove_cursive_style(img)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    return img\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF6yKTu6wNE",
        "colab_type": "text"
      },
      "source": [
        "## NMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWEvbS1c4JqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def should_merge(region, i, j):\n",
        "    neighbor = {(i, j - 1)}\n",
        "    return not region.isdisjoint(neighbor)\n",
        "\n",
        "\n",
        "def region_neighbor(region_set):\n",
        "    region_pixels = np.array(list(region_set))\n",
        "    j_min = np.amin(region_pixels, axis=0)[1] - 1\n",
        "    j_max = np.amax(region_pixels, axis=0)[1] + 1\n",
        "    i_m = np.amin(region_pixels, axis=0)[0] + 1\n",
        "    region_pixels[:, 0] += 1\n",
        "    neighbor = {(region_pixels[n, 0], region_pixels[n, 1]) for n in\n",
        "                range(len(region_pixels))}\n",
        "    neighbor.add((i_m, j_min))\n",
        "    neighbor.add((i_m, j_max))\n",
        "    return neighbor\n",
        "\n",
        "\n",
        "def region_group(region_list):\n",
        "    S = [i for i in range(len(region_list))]\n",
        "    D = []\n",
        "    while len(S) > 0:\n",
        "        m = S.pop(0)\n",
        "        if len(S) == 0:\n",
        "            # S has only one element, put it to D\n",
        "            D.append([m])\n",
        "        else:\n",
        "            D.append(rec_region_merge(region_list, m, S))\n",
        "    return D\n",
        "\n",
        "\n",
        "def rec_region_merge(region_list, m, S):\n",
        "    rows = [m]\n",
        "    tmp = []\n",
        "    for n in S:\n",
        "        if not region_neighbor(region_list[m]).isdisjoint(region_list[n]) or \\\n",
        "                not region_neighbor(region_list[n]).isdisjoint(region_list[m]):\n",
        "            tmp.append(n)\n",
        "    for d in tmp:\n",
        "        S.remove(d)\n",
        "    for e in tmp:\n",
        "        rows.extend(rec_region_merge(region_list, e, S))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def nms2(predict, activation_pixels, threshold=cfg.side_vertex_pixel_threshold):\n",
        "    region_list = []\n",
        "    for i, j in zip(activation_pixels[0], activation_pixels[1]):\n",
        "        merge = False\n",
        "        for k in range(len(region_list)):\n",
        "            if should_merge(region_list[k], i, j):\n",
        "                region_list[k].add((i, j))\n",
        "                merge = True\n",
        "        if not merge:\n",
        "            region_list.append({(i, j)})\n",
        "    D = region_group(region_list)\n",
        "    quad_list = np.zeros((len(D), 4, 2))\n",
        "    score_list = np.zeros((len(D), 4))\n",
        "    for group, g_th in zip(D, range(len(D))):\n",
        "        total_score = np.zeros((4, 2))\n",
        "        for row in group:\n",
        "            for ij in region_list[row]:\n",
        "                score = predict[ij[0], ij[1], 1]\n",
        "                if score >= sigmoid(threshold):\n",
        "                    ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                    if not (sigmoid(cfg.trunc_threshold) <= ith_score < \n",
        "                            sigmoid(1-cfg.trunc_threshold)):\n",
        "                        ith = int(np.around(ith_score))\n",
        "                        total_score[ith * 2:(ith + 1) * 2] += score\n",
        "                        px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                        py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                        p_v = [px, py] + np.reshape(predict[ij[0], ij[1], 3:7],\n",
        "                                              (2, 2))\n",
        "                        quad_list[g_th, ith * 2:(ith + 1) * 2] += score * p_v\n",
        "                    \n",
        "                    #print(sigmoid(cfg.trunc_threshold), ith_score, str(sigmoid(1-cfg.trunc_threshold)))\n",
        "        score_list[g_th] = total_score[:, 0]\n",
        "        quad_list[g_th] /= (total_score + cfg.epsilon)\n",
        "    return score_list, quad_list\n",
        "\n",
        "def nms(predict, activation_pixels, threshold=cfg.side_vertex_pixel_threshold):\n",
        "    region_list = []\n",
        "    for i, j in zip(activation_pixels[0], activation_pixels[1]):\n",
        "        merge   =  False\n",
        "        for k in range(len(region_list)):\n",
        "            if should_merge(region_list[k], i, j):            \n",
        "                region_list[k].add((i, j))\n",
        "                merge = True\n",
        "                \n",
        "        if not merge:\n",
        "            region_list.append({(i, j)})\n",
        "    D = region_group(region_list)\n",
        "    quad_list = np.zeros((len(D), 4, 2))\n",
        "    score_list = np.zeros((len(D), 4))\n",
        "    for group, g_th in zip(D, range(len(D))):\n",
        "        total_score = np.zeros((4, 2))\n",
        "        prev_p_v_0  =  None\n",
        "        prev_p_v_1  =  None   \n",
        "        yellow_list = []\n",
        "        green_list = []\n",
        "        # Firstly find the left-most yellow one and right-most green one\n",
        "        for row in group:\n",
        "            for ij in region_list[row]:\n",
        "                score = predict[ij[0], ij[1], 1]\n",
        "                if score >= threshold:\n",
        "                    ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                    if not (cfg.trunc_threshold <= ith_score < 1 -\n",
        "                            cfg.trunc_threshold):\n",
        "                        ith = int(np.around(ith_score))\n",
        "                        px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                        py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                        if ith == 0:\n",
        "                            if prev_p_v_0 == None:\n",
        "                                prev_p_v_0  = [ px , py ]\n",
        "                            else:\n",
        "                                if prev_p_v_0[0] > px:\n",
        "                                    prev_p_v_0 [ 0 ] =  px\n",
        "                                    prev_p_v_0 [ 1 ] =  py\n",
        "                        if ith == 1:\n",
        "                            if prev_p_v_1 == None:\n",
        "                                prev_p_v_1  = [ px , py ]\n",
        "                            else:\n",
        "                                if prev_p_v_1[0] < px:\n",
        "                                    prev_p_v_1 [ 0 ] =  px\n",
        "                                    prev_p_v_1 [ 1 ] =  py\n",
        "        yellow_list.append(prev_p_v_0)\n",
        "        green_list.append(prev_p_v_1)\n",
        "        \n",
        "        for row in group:\n",
        "            for ij in region_list[row]:\n",
        "                score = predict[ij[0], ij[1], 1]\n",
        "                if score >= threshold:\n",
        "                    ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                    if not (cfg.trunc_threshold <= ith_score < 1 -\n",
        "                            cfg.trunc_threshold):\n",
        "                        ith = int(np.around(ith_score))\n",
        "                        px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                        py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                        if ith == 0:\n",
        "                            FLAG = True\n",
        "                            for item in yellow_list:\n",
        "                                if abs(item[0] - px) > 10:\n",
        "                                    FLAG = FLAG & True\n",
        "                                else:\n",
        "                                    FLAG = FLAG & False\n",
        "                            if FLAG == True:\n",
        "                                yellow_list.append([px, py])\n",
        "                                \n",
        "                        if ith == 1:\n",
        "                            FLAG = True\n",
        "                            for item in green_list:\n",
        "                                if abs(item[0] - px) > 10:\n",
        "                                    FLAG = FLAG & True\n",
        "                                else:\n",
        "                                    FLAG = FLAG & False\n",
        "                            if FLAG == True:\n",
        "                                green_list.append([px, py])\n",
        "        # We divided connected regions into more segments only when # of heads == # of tails \n",
        "        if len(green_list) != 1 and len(yellow_list)!= 1 and len(green_list) == len(yellow_list):\n",
        "            green_list = sorted(green_list)\n",
        "            yellow_list = sorted(yellow_list)\n",
        "            for ls in range(len(green_list)):\n",
        "                prev_p_v_0 = yellow_list[ls]\n",
        "                prev_p_v_1 = green_list[ls]\n",
        "                total_score = np.zeros((4, 2))\n",
        "                tmp = np.zeros((1, 4, 2))\n",
        "                for row in group:\n",
        "                    for ij in region_list[row]:\n",
        "                        score = predict[ij[0], ij[1], 1]\n",
        "                        if score >= threshold:\n",
        "                            ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                            if not (cfg.trunc_threshold <= ith_score < 1 -\n",
        "                                    cfg.trunc_threshold):\n",
        "                                ith = int(np.around(ith_score))\n",
        "                                px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                                py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                                if ith == 0:\n",
        "                                    if prev_p_v_0 == None:\n",
        "                                        prev_p_v_0  = [ px , py ]\n",
        "                                    else:\n",
        "                                        if abs(prev_p_v_0[0] - px) > cfg.segment_region_threshold:\n",
        "                                            continue\n",
        "                                if ith == 1:\n",
        "                                    if prev_p_v_1 == None:\n",
        "                                        prev_p_v_1  = [ px , py ]\n",
        "                                    else:\n",
        "                                        if abs(prev_p_v_1[0] - px) > cfg.segment_region_threshold:\n",
        "                                            continue\n",
        "                                \n",
        "                                total_score[ith * 2:(ith + 1) * 2] += score\n",
        "                                p_v = [px, py] + np.reshape(predict[ij[0], ij[1], 3:7],\n",
        "                                                      (2, 2)) # Expand\n",
        "                                tmp[0, ith * 2:(ith + 1) * 2] += score * p_v\n",
        "                                    \n",
        "                \n",
        "                if not quad_list[g_th].all():\n",
        "                    score_list[g_th] = total_score[:, 0]\n",
        "                    quad_list[g_th] = tmp[0]\n",
        "                    quad_list[g_th] /= (total_score + cfg.epsilon)\n",
        "                else:\n",
        "                    tmp[0] /= (total_score + cfg.epsilon)\n",
        "                    quad_list = np.concatenate((quad_list, tmp))\n",
        "                    score_list = np.concatenate((score_list,np.resize(total_score[:,0], (1,4))))\n",
        "            \n",
        "        else: # We don't have multiple green/yellow boxes in a merged regions\n",
        "            for row in group:\n",
        "                for ij in region_list[row]:\n",
        "                    score = predict[ij[0], ij[1], 1]\n",
        "                    if score >= threshold:\n",
        "                        ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                        if not (cfg.trunc_threshold <= ith_score < 1 -\n",
        "                                cfg.trunc_threshold):\n",
        "                            ith = int(np.around(ith_score))\n",
        "                            px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                            py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                            if ith == 0:\n",
        "                                if prev_p_v_0 == None:\n",
        "                                    prev_p_v_0  = [ px , py ]\n",
        "                                else:\n",
        "                                    if abs(prev_p_v_0[0] - px) > cfg.segment_region_threshold:\n",
        "                                        continue\n",
        "                            if ith == 1:\n",
        "                                if prev_p_v_1 == None:\n",
        "                                    prev_p_v_1  = [ px , py ]\n",
        "                                else:\n",
        "                                    if abs(prev_p_v_1[0] - px) > cfg.segment_region_threshold:\n",
        "                                        continue\n",
        "                            \n",
        "                            total_score[ith * 2:(ith + 1) * 2] += score\n",
        "                            p_v = [px, py] + np.reshape(predict[ij[0], ij[1], 3:7],\n",
        "                                                  (2, 2)) # Expand\n",
        "                            quad_list[g_th, ith * 2:(ith + 1) * 2] += score * p_v\n",
        "                                \n",
        "            score_list[g_th] = total_score[:, 0]\n",
        "            quad_list[g_th] /= (total_score + cfg.epsilon)\n",
        "    return score_list, quad_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVl2GVEW6z4O",
        "colab_type": "text"
      },
      "source": [
        "## Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ItPe8b62gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def binarize (img_path, geo):\n",
        "    Use_MedianFilter = cfg.Use_MedianFilter\n",
        "    # Getting the un-binarized image.\n",
        "    #raw_image=cv2.imread(img_path,0)\n",
        "    raw_image = img_path\n",
        "    if raw_image.shape[0] == 0 or raw_image.shape[1] == 0:\n",
        "        return None \n",
        "    if(len(raw_image.shape)!=2):\n",
        "        grayScale=cv2.cvtColor(raw_image,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
        "    else:\n",
        "        grayScale=raw_image.astype('uint8')\n",
        "    #return grayScale\n",
        "    #Getting the height and width of the Image.\n",
        "    m,n = grayScale.shape\n",
        "    if(cfg.show_process):\n",
        "      print(\"Gray\")\n",
        "      cv2_imshow(grayScale)\n",
        "\n",
        "    high_thresh, thresh_im = cv2.threshold(grayScale, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    lowThresh = 0.5*high_thresh\n",
        "    \n",
        "    #Applying Canny Edge Detector to the Image.\n",
        "    \n",
        "    if(cfg.show_process):\n",
        "      can = cv2.Canny(grayScale,200,100)\n",
        "      print(\"Canny\")\n",
        "      cv2_imshow(can)\n",
        "      \n",
        "    canny_Image = cv2.Canny(grayScale,200,100)\n",
        "    kernel = np.ones((3,3),np.uint8)\n",
        "    #cv2_imshow(canny_Image)\n",
        "    \n",
        "    #Filling the Blobs Using the imFill Function.\n",
        "    \n",
        "    p_min = np.amin(geo, axis=0)\n",
        "    p_max = np.amax(geo, axis=0)\n",
        "    min_xy = p_min.astype(int)\n",
        "    max_xy = p_max.astype(int) + 2\n",
        "    sub_im_arr = canny_Image.copy()\n",
        "    for m in range(min_xy[1], max_xy[1]):\n",
        "        for n in range(min_xy[0], max_xy[0]):\n",
        "            if not point_inside_of_quad(n, m, geo, p_min, p_max):\n",
        "                #print(\"SIZE\",sub_im_arr.shape[0],sub_im_arr.shape[1])\n",
        "                if sub_im_arr.shape[0] > 0 and sub_im_arr.shape[1] > 0 and (m - min_xy[1]) < sub_im_arr.shape[0] and (n - min_xy[0]) < sub_im_arr.shape[1]:\n",
        "                    sub_im_arr[m - min_xy[1], n - min_xy[0]] = 0#sub_im_arr[min_xy[1], min_xy[0],:]# sub_im_arr[] = 255'\n",
        "                    #print(\"HERE: \", m, n)\n",
        "    canny_Image = sub_im_arr \n",
        "    closing = imFill(canny_Image)\n",
        "    if(cfg.show_process):\n",
        "      print(\"Fill\")\n",
        "      cv2_imshow(closing) \n",
        "    \n",
        "    #Finding Index Values in Image Where Pixels are White.  \n",
        "    inds = np.where(closing==255)\n",
        "\n",
        "    #Finding Index Values in Image Where Pixels are Black.  \n",
        "    inds2 = np.where(closing==0)\n",
        "\n",
        "    #Comparing  with original Image to obtain the pixel values for text.  \n",
        "    pix_val = grayScale[inds]\n",
        "\n",
        "    #Comparing  with original Image to obtain the pixel values for back.  \n",
        "    pix_val_back = grayScale[inds2]\n",
        "    \n",
        "    #Taking Median of Text and back and then comparing them.\n",
        "    median_text=np.median(pix_val,axis=0)\n",
        "    median_back=np.median(pix_val_back,axis=0)\n",
        "    if median_text>median_back:\n",
        "        pre_binarize = cv2.bitwise_not(grayScale)\n",
        "    else:\n",
        "        pre_binarize=grayScale\n",
        " \n",
        "    #Sharpen the Image.\n",
        "\n",
        "    sharpen_kernel = np.array([[0,-0.9,0], [-1,4.9,-1], [0,-1,-0]])\n",
        "    new_img = cv2.filter2D(pre_binarize, -1, sharpen_kernel)\n",
        "    if(cfg.show_process):\n",
        "      print(\"Sharpened\")\n",
        "      cv2_imshow(new_img)\n",
        "    #Padding the image to remove unecessary lines. \n",
        "    #new_img = np.pad(pre_binarize,(9,9),\"constant\",constant_values=(255,255))\n",
        "    img= apply_threshold(new_img,thresholder(new_img))\n",
        "    if(cfg.show_process):\n",
        "      print(\"Threshold\")\n",
        "      cv2_imshow(img)\n",
        "\n",
        "    \n",
        "    if Use_MedianFilter:\n",
        "        img = cv2.medianBlur(img,3)\n",
        "        if(cfg.show_process):\n",
        "          print(\"Median\")\n",
        "          cv2_imshow(img)\n",
        "\n",
        "\n",
        "    sub_im_arr = img.copy()\n",
        "    for m in range(min_xy[1], max_xy[1]):\n",
        "        for n in range(min_xy[0], max_xy[0]):\n",
        "            if not point_inside_of_quad(n, m, geo, p_min, p_max):\n",
        "                #print(\"SIZE\",sub_im_arr.shape[0],sub_im_arr.shape[1])\n",
        "                if sub_im_arr.shape[0] > 0 and sub_im_arr.shape[1] > 0 and (m - min_xy[1]) < sub_im_arr.shape[0] and (n - min_xy[0]) < sub_im_arr.shape[1]:\n",
        "                    sub_im_arr[m - min_xy[1], n - min_xy[0]] = 255#sub_im_arr[min_xy[1], min_xy[0],:]# sub_im_arr[] = 255'\n",
        "    img = sub_im_arr\n",
        "    if(cfg.show_process):\n",
        "        print(\"Final: \")\n",
        "    wt, ht, _ = (1024,64,0)\n",
        "    h, w = np.asarray(img).shape[:2]\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    #print(img.shape)\n",
        "    #name = os.path.basename(img_path)\n",
        "    return img\n",
        "\n",
        "def thresholder(img, w_size=15,k=0.5):\n",
        "    # Obtaining rows and cols\n",
        "    rows, cols = img.shape\n",
        "    i_rows, i_cols = rows + 1, cols + 1\n",
        "\n",
        "    # Computing integral images\n",
        "    integ = np.zeros((i_rows, i_cols), np.float)\n",
        "    sqr_integral = np.zeros((i_rows, i_cols), np.float)\n",
        "\n",
        "    integ[1:, 1:] = np.cumsum(np.cumsum(img.astype(np.float), axis=0), axis=1)\n",
        "    sqr_img = np.square(img.astype(np.float))\n",
        "    sqr_integral[1:, 1:] = np.cumsum(np.cumsum(sqr_img, axis=0), axis=1)\n",
        "\n",
        "    # Defining grid\n",
        "    x, y = np.meshgrid(np.arange(1, i_cols), np.arange(1, i_rows))\n",
        "\n",
        "    # Obtaining local coordinates\n",
        "    hw_size = w_size // 2\n",
        "    x1 = (x - hw_size).clip(1, cols)\n",
        "    x2 = (x + hw_size).clip(1, cols)\n",
        "    y1 = (y - hw_size).clip(1, rows)\n",
        "    y2 = (y + hw_size).clip(1, rows)\n",
        "\n",
        "    # Obtaining local areas size\n",
        "    l_size = (y2 - y1 + 1) * (x2 - x1 + 1)\n",
        "\n",
        "    # Computing sums\n",
        "    sums = (integ[y2, x2] - integ[y2, x1 - 1] -\n",
        "            integ[y1 - 1, x2] + integ[y1 - 1, x1 - 1])\n",
        "    sqr_sums = (sqr_integral[y2, x2] - sqr_integral[y2, x1 - 1] -\n",
        "                sqr_integral[y1 - 1, x2] + sqr_integral[y1 - 1, x1 - 1])\n",
        "\n",
        "    # Computing local means\n",
        "    means = sums / l_size\n",
        "\n",
        "    # Computing local standard deviation\n",
        "    stds = np.sqrt(sqr_sums / l_size - np.square(means))\n",
        "\n",
        "    # Computing min and max values\n",
        "    max_std = np.max(stds)\n",
        "    min_v = np.min(img)\n",
        "\n",
        "    # Computing thresholds\n",
        "    thresholds = ((1.0 - k) * means + k * min_v + k * stds /\n",
        "                  max_std * (means - min_v))\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def imFill(im_in):\n",
        "    \n",
        "    # Threshold.\n",
        "    # Set values equal to or above 220 to 0.\n",
        "    # Set values below 220 to 255.\n",
        "    \n",
        "    # th, im_th = cv2.threshold(im_in, 220, 255, cv2.THRESH_BINARY_INV);\n",
        "    th, im_th = cv2.threshold(im_in, 0, 128, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Copy the thresholded image.\n",
        "    im_floodfill = im_th.copy()\n",
        "    \n",
        "    # Mask used to flood filling.\n",
        "    # Notice the size needs to be 2 pixels than the image.\n",
        "    h, w = im_th.shape[:2]\n",
        "    mask = np.zeros((h+2, w+2), np.uint8)\n",
        "    \n",
        "    # Floodfill from point (0, 0)\n",
        "    cv2.floodFill(im_floodfill, mask, (0,0), 255)\n",
        "    \n",
        "    # Invert floodfilled image\n",
        "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
        "\n",
        "    # Combine the two images to get the foreground.\n",
        "    im_out = im_th | im_floodfill_inv\n",
        "\n",
        "    return im_out   \n",
        "def apply_threshold(img, threshold=128, wp_val=255):\n",
        "    #wp_val is the white pixel value.\n",
        "    return ((img >= threshold) * wp_val).astype(np.uint8)\n",
        "   \n",
        "\"\"\"img = image.load_img(\"/content/drive/My Drive/data/demo/photo5.jpg\")\n",
        "\n",
        "#img = Image.open(img_path)\n",
        "#img = image.array_to_img(im2)\n",
        "d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "img = image.img_to_array(img)\n",
        "\n",
        "#img = preprocess_input(img, mode='tf')\n",
        "\n",
        "#b, g, r = img[:,:,0],img[:,:,1],img[:,:,2]\n",
        "#img = np.dstack([r,g,b])\n",
        "out = binarize(img) \n",
        "cv2_imshow(out)\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(img, input_size):\n",
        "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
        "    \n",
        "    if isinstance(img, str):\n",
        "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if isinstance(img, tuple):\n",
        "        image, boundbox = img\n",
        "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        for i in range(len(boundbox)):\n",
        "            if isinstance(boundbox[i], float):\n",
        "                total = len(img) if i < 2 else len(img[0])\n",
        "                boundbox[i] = int(total * boundbox[i])\n",
        "\n",
        "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
        "\n",
        "    wt, ht, _ = input_size\n",
        "    if(len(img.shape)!=2):\n",
        "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
        "    h, w = np.asarray(img).shape\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "    \n",
        "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Not necessary for the KHATTX dataset, uncomment for IAM or RIMES.\n",
        "    \n",
        "    if np.sum(img) * 0.8 > np.sum(binary):\n",
        "        img = illumination_compensation(img)\n",
        "\n",
        "    img = remove_cursive_style(img)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Illumination Compensation based in:\n",
        "    K.-N. Chen, C.-H. Chen, C.-C. Chang,\n",
        "    Efficient illumination compensation techniques for text images, in\n",
        "    Digital Signal Processing.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def illumination_compensation(img):\n",
        "    \"\"\"Illumination compensation technique for text image\"\"\"\n",
        "\n",
        "    def scale(img):\n",
        "        s = np.max(img) - np.min(img)\n",
        "        res = img / s\n",
        "        res -= np.min(res)\n",
        "        res *= 255\n",
        "        return res\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    height, width = img.shape\n",
        "    sqrt_hw = np.sqrt(height * width)\n",
        "\n",
        "    bins = np.arange(0, 300, 10)\n",
        "    bins[26] = 255\n",
        "    hp = np.histogram(img, bins)\n",
        "    hr = 0\n",
        "    for i in range(len(hp[0])):\n",
        "        if hp[0][i] > sqrt_hw:\n",
        "            hr = i * 10\n",
        "            break\n",
        "\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cei = (img - (hr + 50 * 0.3)) * 2\n",
        "    cei[cei > 255] = 255\n",
        "    cei[cei < 0] = 0\n",
        "\n",
        "    m1 = np.asarray([-1, 0, 1, -2, 0, 2, -1, 0, 1]).reshape((3, 3))\n",
        "    m2 = np.asarray([-2, -1, 0, -1, 0, 1, 0, 1, 2]).reshape((3, 3))\n",
        "    m3 = np.asarray([-1, -2, -1, 0, 0, 0, 1, 2, 1]).reshape((3, 3))\n",
        "    m4 = np.asarray([0, 1, 2, -1, 0, 1, -2, -1, 0]).reshape((3, 3))\n",
        "\n",
        "    eg1 = np.abs(cv2.filter2D(img, -1, m1))\n",
        "    eg2 = np.abs(cv2.filter2D(img, -1, m2))\n",
        "    eg3 = np.abs(cv2.filter2D(img, -1, m3))\n",
        "    eg4 = np.abs(cv2.filter2D(img, -1, m4))\n",
        "\n",
        "    eg_avg = scale((eg1 + eg2 + eg3 + eg4) / 4)\n",
        "\n",
        "    h, w = eg_avg.shape\n",
        "    eg_bin = np.zeros((h, w))\n",
        "    eg_bin[eg_avg >= 30] = 255\n",
        "\n",
        "    h, w = cei.shape\n",
        "    cei_bin = np.zeros((h, w))\n",
        "    cei_bin[cei >= 60] = 255\n",
        "\n",
        "    h, w = eg_bin.shape\n",
        "    tli = 255 * np.ones((h, w))\n",
        "    tli[eg_bin == 255] = 0\n",
        "    tli[cei_bin == 255] = 0\n",
        "\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    erosion = cv2.erode(tli, kernel, iterations=1)\n",
        "    int_img = np.asarray(cei)\n",
        "\n",
        "    estimate_light_distribution(width, height, erosion, cei, int_img)\n",
        "\n",
        "    mean_filter = 1 / 121 * np.ones((11, 11), np.uint8)\n",
        "    ldi = cv2.filter2D(scale(int_img), -1, mean_filter)\n",
        "\n",
        "    result = np.divide(cei, ldi) * 260\n",
        "    result[erosion != 0] *= 1.5\n",
        "    result[result < 0] = 0\n",
        "    result[result > 255] = 255\n",
        "\n",
        "    return np.asarray(result, dtype=np.uint8)\n",
        "\n",
        "\n",
        "def estimate_light_distribution(width, height, erosion, cei, int_img):\n",
        "    \"\"\"Light distribution performed by numba\"\"\"\n",
        "\n",
        "    for y in range(width):\n",
        "        for x in range(height):\n",
        "            if erosion[x][y] == 0:\n",
        "                i = x\n",
        "\n",
        "                while i < erosion.shape[0] and erosion[i][y] == 0:\n",
        "                    i += 1\n",
        "\n",
        "                end = i - 1\n",
        "                n = end - x + 1\n",
        "\n",
        "                if n <= 30:\n",
        "                    h, e = [], []\n",
        "\n",
        "                    for k in range(5):\n",
        "                        if x - k >= 0:\n",
        "                            h.append(cei[x - k][y])\n",
        "\n",
        "                        if end + k < cei.shape[0]:\n",
        "                            e.append(cei[end + k][y])\n",
        "\n",
        "                    mpv_h, mpv_e = max(h), max(e)\n",
        "\n",
        "                    for m in range(n):\n",
        "                        int_img[x + m][y] = mpv_h + (m + 1) * ((mpv_e - mpv_h) / n)\n",
        "\n",
        "                x = end\n",
        "                break\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Deslating image process based in,\n",
        "    A. Vinciarelli and J. Luettin,\n",
        "    A New Normalization Technique for Cursive Handwritten Words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def remove_cursive_style(img):\n",
        "    \"\"\"Remove cursive writing style from image with deslanting algorithm\"\"\"\n",
        "\n",
        "    def calc_y_alpha(vec):\n",
        "        indices = np.where(vec > 0)[0]\n",
        "        h_alpha = len(indices)\n",
        "\n",
        "        if h_alpha > 0:\n",
        "            delta_y_alpha = indices[h_alpha - 1] - indices[0] + 1\n",
        "\n",
        "            if h_alpha == delta_y_alpha:\n",
        "                return h_alpha * h_alpha\n",
        "        return 0\n",
        "\n",
        "    alpha_vals = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "    rows, cols = img.shape\n",
        "    results = []\n",
        "\n",
        "    ret, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    binary = otsu if ret < 127 else sauvola(img, (int(img.shape[0] / 2), int(img.shape[0] / 2)), 127, 1e-2)\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "        shift_x = max(-alpha * rows, 0.)\n",
        "        size = (cols + int(np.ceil(abs(alpha * rows))), rows)\n",
        "        transform = np.asarray([[1, alpha, shift_x], [0, 1, 0]], dtype=np.float)\n",
        "\n",
        "        shear_img = cv2.warpAffine(binary, transform, size, cv2.INTER_NEAREST)\n",
        "        sum_alpha = 0\n",
        "        sum_alpha += np.apply_along_axis(calc_y_alpha, 0, shear_img)\n",
        "        results.append([np.sum(sum_alpha), size, transform])\n",
        "\n",
        "    result = sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
        "    warp = cv2.warpAffine(img, result[2], result[1], borderValue=255)\n",
        "\n",
        "    return cv2.resize(warp, dsize=(cols, rows))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Sauvola binarization\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def sauvola(img, window, thresh, k):\n",
        "    \"\"\"Sauvola binarization\"\"\"\n",
        "\n",
        "    rows, cols = img.shape\n",
        "    pad = int(np.floor(window[0] / 2))\n",
        "    sum2, sqsum = cv2.integral2(\n",
        "        cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_CONSTANT))\n",
        "\n",
        "    isum = sum2[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
        "        sum2[0:rows, 0:cols] - \\\n",
        "        sum2[window[0]:rows + window[0], 0:cols] - \\\n",
        "        sum2[0:rows, window[1]:cols + window[1]]\n",
        "\n",
        "    isqsum = sqsum[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
        "        sqsum[0:rows, 0:cols] - \\\n",
        "        sqsum[window[0]:rows + window[0], 0:cols] - \\\n",
        "        sqsum[0:rows, window[1]:cols + window[1]]\n",
        "\n",
        "    ksize = window[0] * window[1]\n",
        "    mean = isum / ksize\n",
        "    std = (((isqsum / ksize) - (mean**2) / ksize) / ksize) ** 0.5\n",
        "    threshold = (mean * (1 + k * (std / thresh - 1))) * (mean >= 100)\n",
        "\n",
        "    return np.asarray(255 * (img >= threshold), 'uint8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO4YjCx04K5a",
        "colab_type": "text"
      },
      "source": [
        "# Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0_MjwpcKwma",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJg4wxalKyVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Gated implementations\n",
        "    GatedConv2D: Introduce a Conv2D layer (same number of filters) to multiply with its sigmoid activation.\n",
        "    FullGatedConv2D: Introduce a Conv2D to extract features (linear and sigmoid), making a full gated process.\n",
        "                     This process will double number of filters to make one convolutional process.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Activation\n",
        "\n",
        "\n",
        "class GatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GatedConv2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(GatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(inputs)\n",
        "        sigmoid = Activation(\"sigmoid\")(output)\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(GatedConv2D, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Tensorflow Keras layer implementation of the gated convolution.\n",
        "    Args:\n",
        "        filters (int): Number of output filters.\n",
        "        kwargs: Other Conv2D keyword arguments.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FullGatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n",
        "        self.nb_filters = filters\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(FullGatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n",
        "        sigmoid = Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Compute shape of layer output\"\"\"\n",
        "\n",
        "        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n",
        "        return tuple(output_shape[:3]) + (self.nb_filters,)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(FullGatedConv2D, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        del config['filters']\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDqOd4i-i7pJ",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yPTuIFwi7IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Handwritten Text Recognition Neural Network\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from contextlib import redirect_stdout\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NNModel Class\n",
        "The NNModel class use Tensorflow 2 Keras module for the use of the\n",
        "Connectionist Temporal Classification (CTC) with the Hadwritten Text Recognition.\n",
        "\n",
        "x is the input features and y the labels.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class NNModel_rec:\n",
        "\n",
        "    def __init__(self,\n",
        "                 architecture,\n",
        "                 input_size,\n",
        "                 vocab_size,\n",
        "                 greedy=False,\n",
        "                 beam_width=10,\n",
        "                 top_paths=1):\n",
        "        \"\"\"\n",
        "        Initialization of a NN Model.\n",
        "\n",
        "        parameters:\n",
        "            architecture: option of the architecture model to build and compile\n",
        "            greedy, beam_width, top_paths: Parameters of the CTC decoding\n",
        "        \"\"\"\n",
        "\n",
        "        self.architecture = globals()[architecture]\n",
        "        self.input_size = input_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.model = None\n",
        "        self.greedy = greedy\n",
        "        self.beam_width = beam_width\n",
        "        self.top_paths = max(1, top_paths)\n",
        "    def summary(self, output=None, target=None):\n",
        "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        if target is not None:\n",
        "            os.makedirs(output, exist_ok=True)\n",
        "\n",
        "            with open(os.path.join(output, target), \"w\") as f:\n",
        "                with redirect_stdout(f):\n",
        "                    self.model.summary()\n",
        "\n",
        "    def load_checkpoint(self, target):\n",
        "        \"\"\" Load a model with checkpoint file\"\"\"\n",
        "\n",
        "        if os.path.isfile(target):\n",
        "            if self.model is None:\n",
        "                self.compile()\n",
        "\n",
        "            self.model.load_weights(target)\n",
        "\n",
        "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
        "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
        "\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                filename=os.path.join(logdir, \"epochs.log\"),\n",
        "                separator=\";\",\n",
        "                append=True),\n",
        "            TensorBoard(\n",
        "                log_dir=logdir,\n",
        "                histogram_freq=10,\n",
        "                profile_batch=0,\n",
        "                write_graph=True,\n",
        "                write_images=False,\n",
        "                update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(\n",
        "                filepath=checkpoint,\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=verbose),\n",
        "            EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                patience=20, # Change this if the training keeps stopping, happened to me earlier.\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                factor=0.2,\n",
        "                patience=15,\n",
        "                verbose=verbose)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def compile(self, learning_rate=None):\n",
        "        \"\"\"\n",
        "        Configures the NN Model for training/predict.\n",
        "\n",
        "        optimizer: optimizer for training\n",
        "        \"\"\"\n",
        "\n",
        "        # define inputs, outputs and optimizer of the chosen architecture\n",
        "        outs = self.architecture(self.input_size, self.vocab_size + 1, learning_rate)\n",
        "        inputs, outputs, optimizer = outs\n",
        "\n",
        "        # create and compile\n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            validation_split=0.0,\n",
        "            validation_data=None,\n",
        "            shuffle=True,\n",
        "            class_weight=None,\n",
        "            sample_weight=None,\n",
        "            initial_epoch=0,\n",
        "            steps_per_epoch=None,\n",
        "            validation_steps=None,\n",
        "            validation_freq=1,\n",
        "            max_queue_size=10,\n",
        "            workers=1,\n",
        "            use_multiprocessing=False,\n",
        "            **kwargs):\n",
        "        \"\"\"\n",
        "        Model training on data yielded (fit function has support to generator).\n",
        "        A fit() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding (x, y, sample_weight).\n",
        "\n",
        "        return: A history object\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                             callbacks=callbacks, validation_split=validation_split,\n",
        "                             validation_data=validation_data, shuffle=shuffle,\n",
        "                             class_weight=class_weight, sample_weight=sample_weight,\n",
        "                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n",
        "                             validation_steps=validation_steps, validation_freq=validation_freq,\n",
        "                             max_queue_size=max_queue_size, workers=workers,\n",
        "                             use_multiprocessing=use_multiprocessing, **kwargs)\n",
        "        return out\n",
        "\n",
        "    def predict(self,\n",
        "                x,\n",
        "                batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False,\n",
        "                ctc_decode=True):\n",
        "        \"\"\"\n",
        "        Model predicting on data yielded (predict function has support to generator).\n",
        "        A predict() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding [x].\n",
        "\n",
        "        returns: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
        "        \"\"\"\n",
        "\n",
        "        #self.model._make_predict_function()\n",
        "\n",
        "        if verbose == 1:\n",
        "            print(\"Model Predict\")\n",
        "\n",
        "        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
        "                                 callbacks=callbacks, max_queue_size=max_queue_size,\n",
        "                                 workers=workers, use_multiprocessing=use_multiprocessing)\n",
        "\n",
        "        if not ctc_decode:\n",
        "            return np.log(out.clip(min=1e-8))\n",
        "\n",
        "        steps_done = 0\n",
        "        if verbose == 1:\n",
        "            print(\"CTC Decode\")\n",
        "            progbar = tf.keras.utils.Progbar(target=steps)\n",
        "\n",
        "        batch_size = int(np.ceil(len(out) / steps))\n",
        "        input_length = len(max(out, key=len))\n",
        "\n",
        "        predicts, probabilities = [], []\n",
        "\n",
        "        while steps_done < steps:\n",
        "            index = steps_done * batch_size\n",
        "            until = index + batch_size\n",
        "\n",
        "            x_test = np.asarray(out[index:until])\n",
        "            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
        "\n",
        "            decode, log = K.ctc_decode(x_test,\n",
        "                                       x_test_len,\n",
        "                                       greedy=self.greedy,\n",
        "                                       beam_width=self.beam_width,\n",
        "                                       top_paths=self.top_paths)\n",
        "\n",
        "            probabilities.extend([np.exp(x) for x in log])\n",
        "            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
        "            predicts.extend(np.swapaxes(decode, 0, 1))\n",
        "\n",
        "            steps_done += 1\n",
        "            if verbose == 1:\n",
        "                progbar.update(steps_done)\n",
        "\n",
        "        return (predicts, probabilities)\n",
        "\n",
        "    @staticmethod\n",
        "    def ctc_loss_lambda_func(y_true, y_pred):\n",
        "        \"\"\"Function for computing the CTC loss\"\"\"\n",
        "\n",
        "        if len(y_true.shape) > 2:\n",
        "            y_true = tf.squeeze(y_true)\n",
        "\n",
        "        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n",
        "        # output of every model is softmax\n",
        "        # so sum across alphabet_size_1_hot_encoded give 1\n",
        "        #               string_length give string length\n",
        "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
        "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
        "\n",
        "        # y_true strings are padded with 0\n",
        "        # so sum of non-zero gives number of characters in this string\n",
        "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
        "\n",
        "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "        # average loss across all entries in the batch\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Networks to the Handwritten Text Recognition Model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def bluche(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolucional Recurrent Neural Network by Bluche et al.\n",
        "\n",
        "    Reference:\n",
        "        Bluche, T., Messina, R.:\n",
        "        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "    cnn = Reshape((input_size[0] // 2, input_size[1] // 2, input_size[2] * 4))(input_data)\n",
        "\n",
        "    cnn = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    blstm = Dense(units=128, activation=\"tanh\")(blstm)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 4e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "\n",
        "def puigcerver(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Convolutional Recurrent Neural Network by Puigcerver et al.\n",
        "\n",
        "    Reference:\n",
        "        Joan Puigcerver.\n",
        "        Are multidimensional recurrent layers really necessary for handwritten text recognition?\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(input_data)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=80, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "\n",
        "    blstm = Dropout(rate=0.5)(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 3e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "def yazeed(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolutional Recurrent Neural Network by Yazeed Mohi-Eldeen Sabil.\n",
        "    Based on the Flor et al architecture, modified some portions to fit the KHATT dataset. \n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\", kernel_initializer=\"he_uniform\")(input_data)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=16, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=32, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=40, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=40, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=48, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=56, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=56, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "\n",
        "    cnn = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    nb_units = shape[2] * shape[3]\n",
        "\n",
        "    bgru = Reshape((shape[1], nb_units))(cnn)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    bgru = Dense(units=nb_units * 2)(bgru)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(bgru)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 5e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAa9RL9b-GTa",
        "colab_type": "text"
      },
      "source": [
        "## Predict Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqrt5fyw4MpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_text(model,\n",
        "              x,\n",
        "              batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False,\n",
        "                ctc_decode=True):\n",
        "      \"\"\"\n",
        "      Model predicting on data yielded (predict function has support to generator).\n",
        "      A predict() abstration function of TensorFlow 2.\n",
        "\n",
        "      Provide x parameter of the form: yielding [x].\n",
        "\n",
        "      returns: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
        "      \"\"\"\n",
        "      x = normalization(x)\n",
        "      #model._make_predict_function()\n",
        "\n",
        "      \n",
        "      #print(\"Model Predict\")\n",
        "      out = model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
        "                                callbacks=callbacks, max_queue_size=max_queue_size,\n",
        "                                workers=workers, use_multiprocessing=use_multiprocessing)\n",
        "\n",
        "      if not ctc_decode:\n",
        "          return np.log(out.clip(min=1e-8))\n",
        "\n",
        "      steps_done = 0\n",
        "      if verbose == 1:\n",
        "          print(\"CTC Decode\")\n",
        "          progbar = tf.keras.utils.Progbar(target=steps)\n",
        "\n",
        "      batch_size = int(np.ceil(len(out) / steps))\n",
        "      input_length = len(max(out, key=len))\n",
        "\n",
        "      predicts, probabilities = [], []\n",
        "      while steps_done < steps:\n",
        "          index = steps_done * batch_size\n",
        "          until = index + batch_size\n",
        "\n",
        "          x_test = np.asarray(out[index:until])\n",
        "          x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
        "\n",
        "          decode, log = K.ctc_decode(x_test,\n",
        "                                      x_test_len,\n",
        "                                      greedy=cfg.greedy,\n",
        "                                      beam_width=cfg.beam_width,\n",
        "                                      top_paths=cfg.top_paths)\n",
        "\n",
        "          probabilities.extend([np.exp(x) for x in log])\n",
        "          decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
        "          predicts.extend(np.swapaxes(decode, 0, 1))\n",
        "\n",
        "          steps_done += 1\n",
        "          if verbose == 1:\n",
        "              progbar.update(steps_done)\n",
        "      return (predicts, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYC1Uq1-CLf",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGy4XcPy9_Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import cv2\n",
        "import html\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "class Tokenizer():\n",
        "    \"\"\"Manages tokens functions and charset/dictionary properties\"\"\"\n",
        "\n",
        "    def __init__(self, chars, max_text_length=128):\n",
        "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\" # PAD = Blank white space before and after text, UNK = Unknown character\n",
        "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
        "\n",
        "        self.PAD = self.chars.find(self.PAD_TK)\n",
        "        self.UNK = self.chars.find(self.UNK_TK)\n",
        "\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.maxlen = max_text_length\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to vector\"\"\"\n",
        "\n",
        "        #text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "        text = \" \".join(text.split())\n",
        "        encoded = []\n",
        "        for item in text:\n",
        "            index = self.chars.find(item)\n",
        "            index = self.UNK if index == -1 else index\n",
        "            encoded.append(index)\n",
        "        \n",
        "        return np.asarray(encoded)\n",
        "\n",
        "    def decode(self, text):\n",
        "        \"\"\"Decode vector to text\"\"\"\n",
        "\n",
        "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
        "        decoded = self.remove_tokens(decoded)\n",
        "        decoded = text_standardize(decoded)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "    def remove_tokens(self, text):\n",
        "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
        "\n",
        "        return text.replace(self.PAD_TK, \"\")\n",
        "\n",
        "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
        "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
        "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
        "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
        "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
        "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
        "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
        "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
        "\n",
        "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
        "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
        "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
        "\n",
        "\n",
        "def text_standardize(text):\n",
        "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
        "\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
        "\n",
        "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
        "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
        "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
        "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
        "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
        "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
        "\n",
        "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
        "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
        "    text = text.translate(str.maketrans({c: \" \"+str(c)+\" \" for c in string.punctuation}))\n",
        "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9nBpud4ORp",
        "colab_type": "text"
      },
      "source": [
        "# Replacement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6rhfKu-VPGz",
        "colab_type": "text"
      },
      "source": [
        "## Put Box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6HAt0bM4Qtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw \n",
        "import arabic_reshaper\n",
        "from keras.preprocessing import image as image\n",
        "\n",
        "\"\"\"img = Image.open(\"sample_in.jpg\")\n",
        "draw = ImageDraw.Draw(img)\n",
        "# font = ImageFont.truetype(<font-file>, <font-size>)\n",
        "font = ImageFont.truetype(\"sans-serif.ttf\", 16)\n",
        "# draw.text((x, y),\"Sample Text\",(r,g,b))\n",
        "draw.text((0, 0),\"Sample Text\",(255,255,255),font=font)\n",
        "img.save('sample-out.jpg')\"\"\"\n",
        "\n",
        "def replace_all(img_path, geo_list, text_list):\n",
        "    img = image.load_img(img_path)\n",
        "\n",
        "    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "    img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    img = image.img_to_array(img)\n",
        "    \n",
        "    #img = preprocess_input(img, mode='tf')\n",
        "\n",
        "    b, g, r = img[:,:,0],img[:,:,1],img[:,:,2]\n",
        "    img = np.dstack([r,g,b])\n",
        "\n",
        "    for geo, text in zip(geo_list, text_list):\n",
        "        img = replace_box(img,geo,text)\n",
        "    cv2_imshow(img)\n",
        "def replace_box(img,geo,text):\n",
        "    if(text == \"#\" or text == \" \" or text == \"\"):\n",
        "        return img\n",
        "    #_, geo, _ = shrink(geo,ratio = cfg.expand_ratio)\n",
        "    geo = square_geo(geo)\n",
        "    p_min = np.amin(geo, axis=0)\n",
        "    p_max = np.amax(geo, axis=0)\n",
        "    min_xy = p_min.astype(int)\n",
        "    max_xy = p_max.astype(int) + 2\n",
        "    sub_im_arr = img\n",
        "    for m in range(min_xy[1], max_xy[1]):\n",
        "        for n in range(min_xy[0], max_xy[0]):\n",
        "            if point_inside_of_quad(n, m, geo, p_min, p_max):\n",
        "                #print(\"SIZE\",sub_im_arr.shape[0],sub_im_arr.shape[1])\n",
        "                if sub_im_arr.shape[0] > 0 and sub_im_arr.shape[1] > 0 and (m - min_xy[1]) < sub_im_arr.shape[0] and (n - min_xy[0]) < sub_im_arr.shape[1]:\n",
        "                    sub_im_arr[m, n, :] = 255\n",
        "    img = sub_im_arr\n",
        "    #img = image.array_to_img(img)\n",
        "    ## FOR ENGLISH TEXT\n",
        "    Arabic = CheckLang(text)\n",
        "    if(not Arabic):\n",
        "      box_height = max_xy[1]-min_xy[1]\n",
        "      font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
        "      bottomLeftCornerOfText = (int(min_xy[0]+0.5*box_height),int(max_xy[1]-0.25*box_height))\n",
        "      fontScale              = box_height /60\n",
        "      fontColor              = (0,0,0)\n",
        "      thickness              = 2\n",
        "      lineType               = cv2.LINE_AA\n",
        "\n",
        "      #image = cv2.imread(path) \n",
        "      \n",
        "\n",
        "      cv2.putText(img,translate(text), \n",
        "      bottomLeftCornerOfText, \n",
        "      font, \n",
        "      fontScale,\n",
        "      fontColor,thickness)\n",
        "\n",
        "    ## FOR ARABIC TEXT\n",
        "    else:\n",
        "      b,g,r,a = 0,0,0,0\n",
        "      fontpath = \"/content/drive/My Drive/data/fonts/tradbdo.ttf\"\n",
        "      box_height = max_xy[1]-min_xy[1]\n",
        "      font = ImageFont.truetype(fontpath, box_height)\n",
        "      img_pil = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n",
        "      #img_pil = Image.fromarray(img)\n",
        "      draw = ImageDraw.Draw(img_pil)\n",
        "      text_to_be_reshaped = text\n",
        "      reshaped_text = arabic_reshaper.reshape(text_to_be_reshaped)\n",
        "      print_text = ''.join(reversed(reshaped_text))\n",
        "      draw.text((min_xy[0]+0.5*box_height,min_xy[1]-0.25*box_height),  print_text, font = font, fill = (b, g, r, a))\n",
        "      img = np.array(img_pil)\n",
        "    return img\n",
        "\n",
        "def CheckLang(text):\n",
        "    return not cfg.translate_result\n",
        "    \n",
        "    \"\"\"draw = ImageDraw.Draw(img)\n",
        "    # font = ImageFont.truetype(<font-file>, <font-size>)\n",
        "    font = ImageFont.truetype(\"sans-serif.ttf\", 16)\n",
        "    # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
        "    draw.text((min_xy[0], min_xy[1]),text,(0,0,255),font=font)\"\"\"\n",
        "#replace_all(\"/content/drive/My Drive/data/demo/sign1.jpg\",[],[]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83Tgi06VRFZ",
        "colab_type": "text"
      },
      "source": [
        "## Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPEkfTC5VTq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import re\n",
        "\n",
        "if (sys.version_info[0] < 3):\n",
        "    import urllib2\n",
        "    import urllib\n",
        "    import HTMLParser\n",
        "else:\n",
        "    import html.parser\n",
        "    import urllib.request\n",
        "    import urllib.parse\n",
        "\n",
        "agent = {'User-Agent':\n",
        "\"Mozilla/4.0 (\\\n",
        "compatible;\\\n",
        "MSIE 6.0;\\\n",
        "Windows NT 5.1;\\\n",
        "SV1;\\\n",
        ".NET CLR 1.1.4322;\\\n",
        ".NET CLR 2.0.50727;\\\n",
        ".NET CLR 3.0.04506.30\\\n",
        ")\"}\n",
        "\n",
        "\n",
        "def unescape(text):\n",
        "    if (sys.version_info[0] < 3):\n",
        "        parser = HTMLParser.HTMLParser()\n",
        "    else:\n",
        "        parser = html.parser.HTMLParser()\n",
        "    return (parser.unescape(text))\n",
        "\n",
        "\n",
        "def translate(to_translate, to_language=\"auto\", from_language=\"auto\"):\n",
        "    \"\"\"Returns the translation using google translate\n",
        "    you must shortcut the language you define\n",
        "    (French = fr, English = en, Spanish = es, etc...)\n",
        "    if not defined it will detect it or use english by default\n",
        "    Example:\n",
        "    print(translate(\"salut tu vas bien?\", \"en\"))\n",
        "    hello you alright?\n",
        "    \"\"\"\n",
        "    base_link = \"http://translate.google.com/m?hl=%s&sl=%s&q=%s\"#\"translate.google.com/m?tl=%s&sl=%s&q=%s&hl=%s\"\n",
        "    if (sys.version_info[0] < 3):\n",
        "        to_translate = urllib.quote_plus(to_translate)\n",
        "        link = base_link % (to_language, from_language, to_translate, from_language)\n",
        "        request = urllib2.Request(link, headers=agent)\n",
        "        raw_data = urllib2.urlopen(request).read()\n",
        "    else:\n",
        "        to_translate = urllib.parse.quote(to_translate)\n",
        "        link = base_link % (to_language, from_language, to_translate)\n",
        "        request = urllib.request.Request(link, headers=agent)\n",
        "        raw_data = urllib.request.urlopen(request).read()\n",
        "    data = raw_data.decode(\"utf-8\")\n",
        "    expr = r'class=\"t0\">(.*?)<'\n",
        "    re_result = re.findall(expr, data)\n",
        "    if (len(re_result) == 0):\n",
        "        result = \"\"\n",
        "    else:\n",
        "        result = unescape(re_result[0])\n",
        "    return (result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktxOHV6Ib-fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade arabic-reshaper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfOqJUyXb4ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, concatenate, BatchNormalization, Lambda, Input, multiply, add, ZeroPadding2D, Activation, Layer, MaxPooling2D, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "RESIZE_FACTOR = 2\n",
        "\n",
        "def resize_bilinear(x):\n",
        "    return tf.image.resize_bilinear(x, size=[K.shape(x)[1]*RESIZE_FACTOR, K.shape(x)[2]*RESIZE_FACTOR])\n",
        "\n",
        "def resize_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    assert len(shape) == 4\n",
        "    shape[1] *= RESIZE_FACTOR\n",
        "    shape[2] *= RESIZE_FACTOR\n",
        "    return tuple(shape)\n",
        "\n",
        "input_image = Input(shape=(None, None, 3), name='input_image')\n",
        "overly_small_text_region_training_mask = Input(shape=(None, None, 1), name='overly_small_text_region_training_mask')\n",
        "text_region_boundary_training_mask = Input(shape=(None, None, 1), name='text_region_boundary_training_mask')\n",
        "target_score_map = Input(shape=(None, None, 1), name='target_score_map')\n",
        "resnet = ResNet50(input_tensor=input_image, weights='imagenet', include_top=False, pooling=None)\n",
        "print(resnet.summary())\n",
        "x = resnet.get_layer('conv5_block3_out').output\n",
        "\n",
        "x = Lambda(resize_bilinear, name='resize_1')(x)\n",
        "x = concatenate([x, resnet.get_layer('activation_40').output], axis=3)\n",
        "x = Conv2D(128, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Lambda(resize_bilinear, name='resize_2')(x)\n",
        "x = concatenate([x, resnet.get_layer('activation_22').output], axis=3)\n",
        "x = Conv2D(64, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Lambda(resize_bilinear, name='resize_3')(x)\n",
        "x = concatenate([x, ZeroPadding2D(((1, 0),(1, 0)))(resnet.get_layer('activation_10').output)], axis=3)\n",
        "x = Conv2D(32, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "pred_score_map = Conv2D(1, (1, 1), activation=tf.nn.sigmoid, name='pred_score_map')(x)\n",
        "rbox_geo_map = Conv2D(4, (1, 1), activation=tf.nn.sigmoid, name='rbox_geo_map')(x) \n",
        "rbox_geo_map = Lambda(lambda x: x * input_size)(rbox_geo_map)\n",
        "angle_map = Conv2D(1, (1, 1), activation=tf.nn.sigmoid, name='rbox_angle_map')(x)\n",
        "angle_map = Lambda(lambda x: (x - 0.5) * np.pi / 2)(angle_map)\n",
        "pred_geo_map = concatenate([rbox_geo_map, angle_map], axis=3, name='pred_geo_map')\n",
        "\n",
        "model = Model(inputs=[input_image, overly_small_text_region_training_mask, text_region_boundary_training_mask, target_score_map], outputs=[pred_score_map, pred_geo_map])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHPj-Iib8eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}